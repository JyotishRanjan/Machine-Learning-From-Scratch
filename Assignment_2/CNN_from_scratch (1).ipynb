{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVymzqkPqtok",
        "outputId": "15756941-5a38-4680-a113-9a4bc280e07d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jXao9Vu4uPKD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9TFx70D3BkP",
        "outputId": "62037b10-1041-43e2-b81f-a19062b4b62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-train-imgs.npz - 18.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17954/17954 [00:15<00:00, 1175.53KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-train-labels.npz - 0.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:00<00:00, 207.55KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-test-imgs.npz - 3.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3008/3008 [00:03<00:00, 925.68KB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading kmnist-test-labels.npz - 0.0 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 3811.27KB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dataset files downloaded!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    tqdm = lambda x, total, unit: x  # If tqdm doesn't exist, replace it with a function that does nothing\n",
        "    print('**** Could not import tqdm. Please install tqdm for download progressbars! (pip install tqdm) ****')\n",
        "\n",
        "download_dict = {\n",
        "    '1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)': {\n",
        "        '1) MNIST data format (ubyte.gz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz'],\n",
        "        '2) NumPy data format (.npz)':\n",
        "            ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz',\n",
        "             'http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz'],\n",
        "    }\n",
        "}\n",
        "\n",
        "# Download a list of files\n",
        "def download_list(url_list):\n",
        "    for url in url_list:\n",
        "        path = url.split('/')[-1]\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(path, 'wb') as f:\n",
        "            total_length = int(r.headers.get('content-length'))\n",
        "            print('Downloading {} - {:.1f} MB'.format(path, (total_length / 1024000)))\n",
        "\n",
        "            for chunk in tqdm(r.iter_content(chunk_size=1024), total=int(total_length / 1024) + 1, unit=\"KB\"):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "    print('All dataset files downloaded!')\n",
        "\n",
        "def traverse_dict(d):\n",
        "    if isinstance(d, list):  # If we've hit a list of downloads, download that list\n",
        "        download_list(d)\n",
        "    else:\n",
        "        selected = list(d.keys())[0]  # Select the first option by default\n",
        "        traverse_dict(d[selected])     # Repeat with the next level\n",
        "\n",
        "traverse_dict(download_dict['1) Kuzushiji-MNIST (10 classes, 28x28, 70k examples)']['2) NumPy data format (.npz)'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSAEvI4Z5Xfi",
        "outputId": "d26c9500-1d07-44eb-ff1c-4f89beacd98a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "X_train = np.load('kmnist-train-imgs.npz')['arr_0']\n",
        "y_train = np.load('kmnist-train-labels.npz')['arr_0']\n",
        "\n",
        "X_test = np.load('kmnist-test-imgs.npz')['arr_0']\n",
        "y_test = np.load('kmnist-test-labels.npz')['arr_0']\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "permutation = np.random.permutation(len(X_train))\n",
        "X_train = X_train[permutation]\n",
        "y_train = y_train[permutation]\n",
        "\n",
        "permutation = np.random.permutation(len(X_test))\n",
        "X_test = X_test[permutation]\n",
        "y_test = y_test[permutation]"
      ],
      "metadata": {
        "id": "Nzv9aL1H5DTl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Range of pixel value --> [-0.5 , 0.5]\n",
        "X_train -= 0.5\n",
        "X_test -= 0.5"
      ],
      "metadata": {
        "id": "_2lLBGngdNei"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1lhKbLn_Zj7"
      },
      "source": [
        "#Architecture:\n",
        "\n",
        "1. **Input Layer**:\n",
        "   - The input layer is responsible for passing the input data to the subsequent layers.\n",
        "\n",
        "\n",
        "2. **Convolutional Layer**:\n",
        "   - This layer performs convolution operations on the input data using learnable filters (kernels).\n",
        "\n",
        "3. **ReLU Layer (Rectified Linear Unit)**:\n",
        "   - The ReLU layer introduces non-linearity into the network by applying the ReLU activation function to the feature maps.\n",
        "\n",
        "4. **Pooling Layer**:\n",
        "   - The pooling layer reduces the spatial dimensions of the feature maps generated by the convolutional layer.\n",
        "\n",
        "5. **Reshaping Layer**:\n",
        "   - The reshaping layer reshapes the output of the preceding layers into a format suitable for feeding into fully connected layers.\n",
        "\n",
        "6. **Fully Connected (Linear) Layers**:\n",
        "   - These layers consist of neurons that are fully connected to all neurons in the previous layer.\n",
        "   \n",
        "7. **Softmax Layer**:\n",
        "   - It computes the probabilities of each class given the input and ensures that the sum of these probabilities is 1.\n",
        "\n",
        "8. **Loss Function (Cross Entropy)**:\n",
        "   - The cross-entropy loss function is used to measure the difference between the predicted probability distribution and the actual distribution (one-hot encoded labels).\n",
        "\n",
        "9. **Accuracy Calculation**:\n",
        "    - The accuracy module calculates the accuracy of the model predictions by comparing the predicted class labels with the true class labels.\n",
        "\n",
        "10. **Training Loop**:\n",
        "    - The training loop runs for multiple epochs, where each epoch consists of iterations over batches of training data. In each iteration, forward pass, backward pass (backpropagation), and optimization (applying SGD) are performed to update the parameters of the network.\n",
        "    - Learning rate adjustments based on performance thresholds are also implemented to improve convergence and accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_Network:\n",
        "\n",
        "    def __init__(self, Network):\n",
        "        self.Network = Network\n",
        "\n",
        "    # forward pass\n",
        "    def forward_pass(self, X):\n",
        "        n = X\n",
        "        for i in self.Network:\n",
        "            n = i.forward_pass(n,saved_weights = None)\n",
        "        return n\n",
        "\n",
        "    #backward pass\n",
        "    def backprop(self, Y):\n",
        "        m = Y\n",
        "        for i in (reversed(self.Network)):\n",
        "            m = i.backprop(m)\n",
        "\n",
        "    # applying sgd\n",
        "    def applying_sgd(self):\n",
        "        for i in self.Network:\n",
        "            i.applying_sgd()\n",
        "\n",
        "    # applying adam\n",
        "    def applying_adam(self):\n",
        "        for i in self.Network:\n",
        "            i.applying_adam()\n",
        "\n",
        "    # changing alpha\n",
        "    def change_alpha(self):\n",
        "        for i in self.Network:\n",
        "            i.change_alpha()\n",
        "\n",
        "    # saving weights\n",
        "    def saving_params(self):\n",
        "        for i,layer in enumerate(self.Network):\n",
        "            layer.saving_params()\n",
        "\n",
        "    # predicting after loading weights\n",
        "    def predict(self,X):\n",
        "        n = X\n",
        "        for i in self.Network:\n",
        "            n = i.forward_pass(n,saved_weights = 1)\n",
        "        return n\n"
      ],
      "metadata": {
        "id": "Jy-EOjAljNhR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADAM Optimizer"
      ],
      "metadata": {
        "id": "FvRsLLfAwCYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        self.t = 0\n",
        "\n",
        "    def update(self, grads):\n",
        "        if self.m is None:\n",
        "            self.m = np.zeros_like(grads)\n",
        "            self.v = np.zeros_like(grads)\n",
        "\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)\n",
        "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
        "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
        "        return self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
      ],
      "metadata": {
        "id": "yq7OnpDtwIt6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ACCURACY"
      ],
      "metadata": {
        "id": "bsAOwuleuTP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g9H6H1SatxZ8"
      },
      "outputs": [],
      "source": [
        "class accuracy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def value(self, out, Y):\n",
        "        self.out = np.argmax(out, axis=1)\n",
        "        return np.mean(self.out == Y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FLATTEN"
      ],
      "metadata": {
        "id": "pFA4x5xluYu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Forward prop --> ( n , a , b , c ) --> ( n , a * b * c )\n",
        "\n",
        "\n",
        "\n",
        "*   Backward Prop --> ( n , a * b * c ) --> ( n , a , b , c )\n",
        "\n"
      ],
      "metadata": {
        "id": "HRh1AdlStVV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class reshaping:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_pass(self, data, saved_weights = None):\n",
        "        self.data_shape = data.shape\n",
        "\n",
        "        self.flatten = data.reshape(self.data_shape[0], self.data_shape[1]*self.data_shape[2]*self.data_shape[3])\n",
        "        print(self.flatten.shape)\n",
        "        return self.flatten\n",
        "\n",
        "    def backprop(self, prev_data):\n",
        "        return prev_data.reshape(self.data_shape[0], self.data_shape[1], self.data_shape[2], self.data_shape[3])\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def change_alpha(self):\n",
        "        pass\n",
        "\n",
        "    def applying_adam(self):\n",
        "        pass\n",
        "\n",
        "    def saving_params(self):\n",
        "        pass\n",
        "\n"
      ],
      "metadata": {
        "id": "V7xVcXYTyn7Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CROSS ENTROPY\n",
        "\n",
        "L = - sum ( y_i * log(p_i) )"
      ],
      "metadata": {
        "id": "GY1FXpehudTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cross_entropy:\n",
        "\n",
        "    def _init_(self):\n",
        "        pass\n",
        "\n",
        "    def loss(self, A, Y):\n",
        "\n",
        "        epsilon = 1e-15  # Small value to prevent division by zero\n",
        "\n",
        "        # Compute cross-entropy loss\n",
        "        m = Y.shape[0]  # Number of examples\n",
        "        ce_loss = -np.sum(np.log(A[np.arange(m), Y] + epsilon)) / m\n",
        "\n",
        "        return ce_loss"
      ],
      "metadata": {
        "id": "TPEO8GE_H_L6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pyJ0Dyeo_Kc"
      },
      "source": [
        "# SoftMax\n",
        "\n",
        "1. forward_pass(x) :  Using formula. returns softmax_probs\n",
        "2. backward_prop(actual_y) : returns gradient = softmax_probs - one_hot_encoder(actual)\n",
        "3. expansion(actual_y) : returns one hot vector of actual_y\n",
        "\n",
        "[Softmax Backpropogation](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)\n",
        "\n",
        "[Stable Softmax](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class softmax:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def expansion(self, actual_pred):\n",
        "        d = actual_pred.shape[0]\n",
        "        one_hot_pred = np.zeros((d,self.n_classes))\n",
        "        for i in range(0,d):\n",
        "            one_hot_pred[i,actual_pred[i]] = 1\n",
        "        return one_hot_pred\n",
        "\n",
        "    def forward_pass(self, z, saved_weights = None):\n",
        "\n",
        "        self.n_classes = z.shape[1]\n",
        "        # vectorised form below\n",
        "        shiftx = z - np.max(z, axis=1, keepdims=True)\n",
        "\n",
        "        # Exponentiate the shifted values\n",
        "        exps = np.exp(shiftx)\n",
        "\n",
        "        # Calculate softmax probabilities\n",
        "        self.softmax_probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "        return self.softmax_probs\n",
        "\n",
        "        # self.a = np.zeros_like(z)\n",
        "        # \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n",
        "        # for i,x in enumerate(z):\n",
        "        #   shiftx = x - np.max(x)\n",
        "        #   exps = np.exp(shiftx)\n",
        "        #   self.a[i] = exps / np.sum(exps)\n",
        "        # return self.a\n",
        "\n",
        "    def backprop(self, Y):\n",
        "        y = self.expansion(Y)\n",
        "        self.grad = (self.softmax_probs - y)\n",
        "        return self.grad\n",
        "\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def change_alpha(self):\n",
        "        pass\n",
        "\n",
        "    def applying_adam(self):\n",
        "        pass\n",
        "\n",
        "    def saving_params(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAe-iE-64EAj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RELU\n",
        "\n",
        "\n",
        "\n",
        "*   forward --> If element of input_data <= 0 then 0 else same. Returns output\n",
        "*   backward --> Take input of forward pass. Replace positive values with 1 and rest 0. Then do elementwise multiplication Gradient of successive layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "uQ4WjNO0ubjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class relu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward_pass(self, z, saved_weights = None):\n",
        "        self.z = z\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    def derivative(self, a):\n",
        "        return np.where(a > 0, 1, 0)\n",
        "\n",
        "    def backprop(self, grad_previous):\n",
        "        return grad_previous * self.derivative(self.z)\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def change_alpha(self):\n",
        "        pass\n",
        "\n",
        "    def applying_adam(self):\n",
        "        pass\n",
        "\n",
        "    def saving_params(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "C6AYYJwNu11u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#POOLING\n",
        "\n",
        "\n",
        "*   forward pass --> ( n , a , b , c ) --> ( n , a , b/2 , c/2 )\n",
        "Taking only max value from each pooling window.\n",
        "*   Backward pass --> ( n , a , b/2 , c/2 ) --> ( n , a , b , c )\n",
        "Passing gradient to only those elements where max was found on that pooling window.\n",
        "\n"
      ],
      "metadata": {
        "id": "0uZcpT-qmizr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class pooling:\n",
        "    def __init__(self, pool_size=(2, 2), strides=None):\n",
        "        self.pool_height, self.pool_width = pool_size\n",
        "        if strides is None:\n",
        "            self.strides = pool_size\n",
        "        else:\n",
        "            self.strides = strides\n",
        "\n",
        "    def forward_pass(self, input_data, saved_weights = None):\n",
        "        self.input_data = input_data\n",
        "        self.input_data_shape = input_data.shape\n",
        "        batch_size, input_channels, input_height, input_width = input_data.shape\n",
        "        output_height = (input_height - self.pool_height) // self.strides[0] + 1\n",
        "        output_width = (input_width - self.pool_width) // self.strides[1] + 1\n",
        "        self.output = np.zeros((batch_size, input_channels, output_height, output_width))\n",
        "\n",
        "        for i in range(output_height):\n",
        "            for j in range(output_width):\n",
        "                self.output[:,:, i, j] = np.max(input_data[:, :, i*self.strides[0]:i*self.strides[0]+self.pool_height,\n",
        "                                            j*self.strides[1]:j*self.strides[1]+self.pool_width],axis = (2,3))\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backprop(self, grad_previous):\n",
        "        batch_size, input_channels, output_height, output_width = grad_previous.shape\n",
        "        grad_back = np.zeros(self.input_data_shape)\n",
        "\n",
        "        for i in range(self.output.shape[2]):\n",
        "          for j in range(self.output.shape[3]):\n",
        "\n",
        "            x_start = i*self.strides[0]\n",
        "            x_end = x_start + self.pool_height\n",
        "            y_start = j * self.strides[1]\n",
        "            y_end = y_start + self.pool_width\n",
        "            grad_back[:, :, x_start:x_end,y_start:y_end] = np.where(self.input_data[:, :, x_start:x_end,y_start:y_end] >= (self.output[:,:,i,j])[:,:,np.newaxis, np.newaxis],(grad_previous[:,:, i, j])[:,:,np.newaxis, np.newaxis],0)\n",
        "\n",
        "        return grad_back\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        pass\n",
        "\n",
        "    def change_alpha(self):\n",
        "        pass\n",
        "\n",
        "    def applying_adam(self):\n",
        "        pass\n",
        "\n",
        "    def saving_params(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4xdrtnuOtA64"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djxwm8IvnVUH"
      },
      "source": [
        "#Linear Layer\n",
        "1. Does Input( n , a ) * Theta( a , 10 ) + Bias( 10 )\n",
        "2. forward prop(x) --> (thetha)x + b\n",
        "3. back_prop(grad_forward) --> grdient wrt x,theta,b. Returns grad_back\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_Layer:\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, alpha = 0.01,index = 0,reg = None, reg_penelty = 0):\n",
        "        self.Theta = np.random.randn(in_dim, out_dim)/(in_dim * out_dim)\n",
        "        self.bias = np.zeros((out_dim,))\n",
        "        self.optimizer_theta = Adam(lr = alpha)\n",
        "        self.optimizer_bias = Adam(lr = alpha)\n",
        "        self.alpha = alpha\n",
        "        self.index = index\n",
        "        self.reg = reg\n",
        "        self.reg_penelty = reg_penelty\n",
        "\n",
        "\n",
        "    def forward_pass(self, X, saved_weights = None):\n",
        "        if saved_weights != None:\n",
        "          saved_data = np.load(f'/content/drive/MyDrive/Colab Notebooks/Saved_Models/Linear_layer{self.index}.npz')\n",
        "          self.Theta =  saved_data['arr1']\n",
        "          self.bias = saved_data['arr2']\n",
        "\n",
        "        self.X = X\n",
        "        self.z = np.dot(X,self.Theta) + self.bias\n",
        "        return self.z\n",
        "\n",
        "\n",
        "    def backprop(self, grad_previous):\n",
        "        t= self.X.shape[0]\n",
        "        self.grad_theta = np.matmul((self.X.transpose()), grad_previous)\n",
        "        self.grad_bias = (grad_previous.sum(axis=0))/t\n",
        "        self.grad_back = np.matmul(grad_previous, self.Theta.transpose())\n",
        "\n",
        "        # Add L1 and L2 and elastic regularization terms\n",
        "        if self.reg == 'l1':\n",
        "          self.grad_theta += self.reg_penelty * np.sign(self.Theta)\n",
        "          self.grad_bias += self.reg_penelty * np.sign(self.bias)\n",
        "\n",
        "        elif self.reg == 'l2':\n",
        "          self.grad_theta += 2 * self.reg_penelty * self.Theta\n",
        "          self.grad_bias += 2 * self.reg_penelty * self.bias\n",
        "\n",
        "        elif self.reg == 'elastic':\n",
        "          self.grad_theta += self.reg_penelty * (0.5 * np.sign(self.Theta) + 0.5 * self.Theta)\n",
        "          self.grad_bias += self.reg_penelty * (0.5 * np.sign(self.bias) + 0.5 * self.bias)\n",
        "\n",
        "        return self.grad_back\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        self.Theta = self.Theta - (self.alpha*self.grad_theta)\n",
        "        self.bias = self.bias - (self.alpha*self.grad_bias)\n",
        "\n",
        "    def applying_adam(self):\n",
        "        self.Theta -= self.optimizer_theta.update(self.grad_theta)\n",
        "        self.bias -= self.optimizer_bias.update(self.grad_bias)\n",
        "\n",
        "    def change_alpha(self):\n",
        "        self.alpha = self.alpha/5\n",
        "\n",
        "    def saving_params(self):\n",
        "        np.savez(f'/content/drive/MyDrive/Colab Notebooks/Saved_Models/Linear_layer{self.index}.npz',arr1 = self.Theta, arr2 = self.bias)"
      ],
      "metadata": {
        "id": "9u56YHip0rT0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONV LAYER\n",
        "\n",
        "Convolution with kernels donw on each image.\n",
        "\n",
        "( n , 28 , 28 ) --> ( n , n_filters , new_height , new_width )"
      ],
      "metadata": {
        "id": "NBGHklk_uiIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolutional_Layer:\n",
        "    def __init__(self, filter_dim = 3, stride = 1, pad = 0, alpha=0.01, num_of_filters = 1,reg = None,reg_penelty = 0):\n",
        "        self.filter_dim = filter_dim\n",
        "        self.n_filters = num_of_filters\n",
        "        self.stride = stride\n",
        "        self.bias = np.random.randn(self.n_filters, 1) / self.n_filters\n",
        "        self.filter = np.random.randn(self.n_filters, self.filter_dim, self.filter_dim)/(self.filter_dim ** 2)\n",
        "        self.pad = pad\n",
        "        self.alpha = alpha\n",
        "        self.optimizer_bias = Adam(lr = alpha)\n",
        "        self.optimizer_filter = Adam(lr = alpha)\n",
        "        self.reg_penelty = reg_penelty\n",
        "        self.reg = reg\n",
        "\n",
        "    def convolving(self, dimen_x, dimen_y):\n",
        "        z = np.zeros((self.X.shape[0],self.n_filters, dimen_x, dimen_y))\n",
        "        for i in range(dimen_x):\n",
        "            for ii in range(dimen_y):\n",
        "                  temp = np.multiply(self.X[:, np.newaxis,i : i+self.filter_dim, ii : ii+self.filter_dim], self.filter[ np.newaxis,:, :, :])\n",
        "                  z[:,:,i,ii] = np.sum(temp,axis=(2,3)) + self.bias[:,0]\n",
        "        return z\n",
        "\n",
        "\n",
        "    def forward_pass(self, X, saved_weights = None):\n",
        "        if saved_weights != None:\n",
        "          saved_data = np.load('/content/drive/MyDrive/Colab Notebooks/Saved_Models/conv2d.npz')\n",
        "          self.filter = saved_data['arr1']\n",
        "          self.bias = saved_data['arr2']\n",
        "\n",
        "        self.X = np.pad(X , ((0, 0), (self.pad, self.pad), (self.pad, self.pad)),'constant', constant_values=0)\n",
        "        (d, p, t) = self.X.shape\n",
        "        dimen_x = int(((p - self.filter_dim)/self.stride) + 1)\n",
        "        dimen_y = int(((t - self.filter_dim)/self.stride) + 1)\n",
        "        self.z = np.zeros((d, self.n_filters, dimen_x, dimen_y))\n",
        "        self.z = self.convolving(dimen_x, dimen_y)\n",
        "\n",
        "        return self.z\n",
        "\n",
        "    def backprop(self, grad_z):\n",
        "        (d, f, p, t) = grad_z.shape\n",
        "\n",
        "        self.grads = np.zeros((d, p, t))\n",
        "        # for i in range(d):\n",
        "        #   for k in range(self.n_filters):\n",
        "        #     filter_1 = np.flip((np.flip(self.filter[k], axis = 0)), axis = 1)\n",
        "        #     self.grads[i] += self.convolving(np.pad(grad_z[i,k], ((1,1), (1,1)), 'constant', constant_values = 0), filter_1, p, t)\n",
        "\n",
        "        # self.grads /= self.n_filters\n",
        "        # self.grads = np.pad(self.grads, ((0,0),(1,1),(1,1)), 'constant', constant_values = 0)\n",
        "\n",
        "        self.grad_filter = np.zeros((self.n_filters, self.filter_dim, self.filter_dim))\n",
        "        grad_z = np.pad(grad_z, ((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)), 'constant', constant_values = 0)\n",
        "\n",
        "        for i in range(self.filter_dim):\n",
        "              for ii in range(self.filter_dim):\n",
        "                  self.grad_filter[:, i, ii] = np.sum(np.multiply(grad_z[:,:,:,:], self.X[:,  np.newaxis, i:p+i, ii:t+ii]),axis =(0,2,3) )\n",
        "        self.grad_filter = self.grad_filter /(grad_z.shape[0] * grad_z.shape[2]*grad_z.shape[3])\n",
        "\n",
        "        self.grad_bias = np.zeros_like(self.bias)\n",
        "\n",
        "        for k in range(self.n_filters):\n",
        "          self.grad_bias[k] = (grad_z[:,k].sum()) /(grad_z.shape[0] * grad_z.shape[2]*grad_z.shape[3])\n",
        "\n",
        "        # Add L1 and L2 and elastic regularization terms\n",
        "        if self.reg == 'l1':\n",
        "          self.grad_filter += self.reg_penelty * np.sign(self.filter)\n",
        "          self.grad_bias += self.reg_penelty * np.sign(self.bias)\n",
        "\n",
        "        elif self.reg == 'l2':\n",
        "          self.grad_filter += 2 * self.reg_penelty * self.filter\n",
        "          self.grad_bias += 2 * self.reg_penelty * self.bias\n",
        "\n",
        "        elif self.reg == 'elastic':\n",
        "          self.grad_filter += self.reg_penelty * (0.5 * np.sign(self.filter) + 0.5 * self.filter)\n",
        "          self.grad_bias += self.reg_penelty * (0.5 * np.sign(self.bias) + 0.5 * self.bias)\n",
        "\n",
        "        return self.grads\n",
        "\n",
        "    def applying_sgd(self):\n",
        "        self.filter = self.filter - (self.alpha*self.grad_filter)\n",
        "        self.bias = self.bias - (self.alpha*self.grad_bias)\n",
        "\n",
        "    def applying_adam(self):\n",
        "        self.filter -= self.optimizer_bias.update(self.grad_filter)\n",
        "        self.bias -= self.optimizer_filter.update(self.grad_bias)\n",
        "\n",
        "    def change_alpha(self):\n",
        "        self.alpha = self.alpha/5\n",
        "\n",
        "    def saving_params(self):\n",
        "        np.savez('/content/drive/MyDrive/Colab Notebooks/Saved_Models/conv2d.npz',arr1 = self.filter,arr2 = self.bias )"
      ],
      "metadata": {
        "id": "_43bmwlMY9r5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING Using Regularisation (Not Tuned)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SYCcchpzrgtl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qP3bc0jA4JjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e15e6d1-7b81-4def-e161-13c89ae601cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6000, 2704)\n",
            "epoch:1 \t batch: 1 \t \taccuracy: 11.766666666666667\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 2 \t \taccuracy: 39.983333333333334\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 3 \t \taccuracy: 69.33333333333334\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 4 \t \taccuracy: 64.98333333333333\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 5 \t \taccuracy: 66.45\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 6 \t \taccuracy: 68.7\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 7 \t \taccuracy: 69.76666666666667\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 8 \t \taccuracy: 72.31666666666666\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 9 \t \taccuracy: 72.56666666666666\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 10 \t \taccuracy: 71.93333333333334\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 1 \t \taccuracy: 72.76666666666667\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 2 \t \taccuracy: 72.06666666666666\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 3 \t \taccuracy: 72.25\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 4 \t \taccuracy: 71.06666666666666\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 5 \t \taccuracy: 71.36666666666667\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 6 \t \taccuracy: 71.5\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 7 \t \taccuracy: 70.85000000000001\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 8 \t \taccuracy: 72.06666666666666\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 9 \t \taccuracy: 73.0\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 10 \t \taccuracy: 71.88333333333333\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 1 \t \taccuracy: 73.21666666666667\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 2 \t \taccuracy: 72.58333333333333\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 3 \t \taccuracy: 72.88333333333334\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 4 \t \taccuracy: 72.05\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 5 \t \taccuracy: 71.53333333333333\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 6 \t \taccuracy: 71.46666666666667\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 7 \t \taccuracy: 70.11666666666667\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 8 \t \taccuracy: 70.86666666666666\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 9 \t \taccuracy: 71.56666666666666\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 10 \t \taccuracy: 70.61666666666667\n"
          ]
        }
      ],
      "source": [
        "X_testing = X_train\n",
        "Y_testing = y_train\n",
        "\n",
        "al = 0.005\n",
        "\n",
        "complete_NN = Neural_Network([\n",
        "                                Convolutional_Layer(alpha = al,num_of_filters = 16,pad = 0, reg = 'l1',reg_penelty = 0.5),\n",
        "                                relu(),\n",
        "                                pooling(),\n",
        "                                reshaping(),\n",
        "                                Linear_Layer(2704, 10, alpha = al,index = 0, reg = 'l2',reg_penelty = 0.5),\n",
        "                                softmax()\n",
        "                                ])\n",
        "CE = cross_entropy()\n",
        "\n",
        "acc = accuracy()\n",
        "epochs = 3\n",
        "batch_size = 6000\n",
        "done = 0\n",
        "for i in range(epochs):\n",
        "    for batch in range(0, X_testing.shape[0], batch_size):\n",
        "        out = complete_NN.forward_pass(X_testing[batch:batch + batch_size,:,:])\n",
        "        print(\"epoch:{} \\t batch: {} \\t \".format(i+1, 1 + (batch//batch_size)), end=\"\\t\")  #, CE.loss(out, Y_testing[batch:batch + batch_size])\n",
        "        accuracy_val = acc.value(out, Y_testing[batch:batch + batch_size])*100\n",
        "        print(\"accuracy: {}\".format(accuracy_val))\n",
        "\n",
        "        # if ((accuracy_val>=80) and (done==0)):\n",
        "        #     complete_NN.change_alpha()\n",
        "        #     done += 1\n",
        "        # if ((accuracy_val>=85) and (done==1)):\n",
        "        #     complete_NN.change_alpha()\n",
        "        #     done += 1\n",
        "\n",
        "        # if ((accuracy_val>=90) and (done==2)):\n",
        "        #     complete_NN.change_alpha()\n",
        "        #     done += 1\n",
        "\n",
        "        # if (accuracy_val>=95):\n",
        "        #     complete_NN.change_alpha()\n",
        "        #     done += 1\n",
        "\n",
        "        complete_NN.backprop(Y_testing[batch:batch + batch_size])\n",
        "        # complete_NN.applying_sgd()\n",
        "        complete_NN.applying_adam()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Regularisation"
      ],
      "metadata": {
        "id": "FeFktkL8MlYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_5 = complete_NN.forward_pass(X_test)\n",
        "print(\"The accuracy on test set is {}\".format(acc.value(out_5, y_test)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqhhpVjyMNhA",
        "outputId": "64183023-4127-43b3-8a89-3491e15f433a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 2704)\n",
            "The accuracy on test set is 53.900000000000006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Without Regularisation"
      ],
      "metadata": {
        "id": "B1LmoC3fMuhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_testing = X_train\n",
        "Y_testing = y_train\n",
        "\n",
        "al = 0.005\n",
        "\n",
        "complete_NN = Neural_Network([\n",
        "                                Convolutional_Layer(alpha = al,num_of_filters = 16,pad = 0),\n",
        "                                relu(),\n",
        "                                pooling(),\n",
        "                                reshaping(),\n",
        "                                Linear_Layer(2704, 10, alpha = al,index = 0),\n",
        "                                softmax()\n",
        "                                ])\n",
        "CE = cross_entropy()\n",
        "\n",
        "acc = accuracy()\n",
        "epochs = 5\n",
        "batch_size = 6000\n",
        "done = 0\n",
        "for i in range(epochs):\n",
        "    for batch in range(0, X_testing.shape[0], batch_size):\n",
        "        out = complete_NN.forward_pass(X_testing[batch:batch + batch_size,:,:])\n",
        "        print(\"epoch:{} \\t batch: {} \\t \".format(i+1, 1 + (batch//batch_size)), end=\"\\t\")  #, CE.loss(out, Y_testing[batch:batch + batch_size])\n",
        "        accuracy_val = acc.value(out, Y_testing[batch:batch + batch_size])*100\n",
        "        print(\"accuracy: {}\".format(accuracy_val))\n",
        "\n",
        "        complete_NN.backprop(Y_testing[batch:batch + batch_size])\n",
        "        # complete_NN.applying_sgd()\n",
        "        complete_NN.applying_adam()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIhsa51mM0Qe",
        "outputId": "c3b3eb09-7756-40ba-8bce-6644cd8c5fe3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6000, 2704)\n",
            "epoch:1 \t batch: 1 \t \taccuracy: 10.533333333333333\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 2 \t \taccuracy: 31.966666666666665\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 3 \t \taccuracy: 56.65\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 4 \t \taccuracy: 57.4\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 5 \t \taccuracy: 67.46666666666667\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 6 \t \taccuracy: 69.58333333333333\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 7 \t \taccuracy: 70.85000000000001\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 8 \t \taccuracy: 71.76666666666667\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 9 \t \taccuracy: 71.83333333333334\n",
            "(6000, 2704)\n",
            "epoch:1 \t batch: 10 \t \taccuracy: 71.1\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 1 \t \taccuracy: 73.18333333333334\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 2 \t \taccuracy: 73.98333333333333\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 3 \t \taccuracy: 75.23333333333333\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 4 \t \taccuracy: 75.1\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 5 \t \taccuracy: 75.6\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 6 \t \taccuracy: 76.6\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 7 \t \taccuracy: 77.71666666666667\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 8 \t \taccuracy: 79.11666666666667\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 9 \t \taccuracy: 80.05\n",
            "(6000, 2704)\n",
            "epoch:2 \t batch: 10 \t \taccuracy: 79.78333333333333\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 1 \t \taccuracy: 80.65\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 2 \t \taccuracy: 80.21666666666667\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 3 \t \taccuracy: 81.28333333333333\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 4 \t \taccuracy: 81.0\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 5 \t \taccuracy: 82.05\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 6 \t \taccuracy: 82.03333333333333\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 7 \t \taccuracy: 82.16666666666667\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 8 \t \taccuracy: 83.23333333333333\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 9 \t \taccuracy: 83.65\n",
            "(6000, 2704)\n",
            "epoch:3 \t batch: 10 \t \taccuracy: 83.03333333333333\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 1 \t \taccuracy: 83.95\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 2 \t \taccuracy: 83.81666666666666\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 3 \t \taccuracy: 84.2\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 4 \t \taccuracy: 84.05\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 5 \t \taccuracy: 84.45\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 6 \t \taccuracy: 84.11666666666666\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 7 \t \taccuracy: 84.6\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 8 \t \taccuracy: 85.58333333333333\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 9 \t \taccuracy: 86.08333333333333\n",
            "(6000, 2704)\n",
            "epoch:4 \t batch: 10 \t \taccuracy: 85.31666666666666\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 1 \t \taccuracy: 86.26666666666667\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 2 \t \taccuracy: 85.91666666666666\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 3 \t \taccuracy: 86.38333333333334\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 4 \t \taccuracy: 86.46666666666667\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 5 \t \taccuracy: 86.83333333333333\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 6 \t \taccuracy: 86.43333333333332\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 7 \t \taccuracy: 86.83333333333333\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 8 \t \taccuracy: 87.81666666666666\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 9 \t \taccuracy: 87.75\n",
            "(6000, 2704)\n",
            "epoch:5 \t batch: 10 \t \taccuracy: 87.43333333333332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Without Regularisation"
      ],
      "metadata": {
        "id": "_BTMIMZCMpTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_3 = complete_NN.forward_pass(X_test)\n",
        "print(\"The accuracy on test set is {}\".format(acc.value(out_3, y_test)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a_IXmnaM4zs",
        "outputId": "db2ef675-3b5f-4dff-9525-fc51133de07c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 2704)\n",
            "The accuracy on test set is 75.66000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SAVING WEIGHTS"
      ],
      "metadata": {
        "id": "oFhaI5NgYg1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# complete_NN.saving_params()"
      ],
      "metadata": {
        "id": "KieL3J_yYfvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClXQfZux4c7y"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}